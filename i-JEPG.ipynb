{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # Randomly flip images horizontally\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    # Convert images to PyTorch tensors and scale to [0, 1]\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "animal_dataset = datasets.ImageFolder(root='/kaggle/input/animals/animals',  # Specify the root directory of the dataset\n",
    "                               transform=transform)  # Apply the defined transformations to the dataset\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(animal_dataset, 32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class GetPatchEmbeddings(nn.Module):\n",
    "    def __init__(self, patch_size=16):\n",
    "        super(GetPatchEmbeddings, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def divide_into_patches(self,image , patch_size=16):\n",
    "        \"\"\"\n",
    "        Divides an image into non-overlapping patches.\n",
    "\n",
    "        Args:\n",
    "            image (torch.Tensor): Input image of shape (C, H, W).\n",
    "            patch_size (int): Size of each square patch (patch_size x patch_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of patches of shape (num_patches, C, patch_size, patch_size).\n",
    "        \"\"\"\n",
    "        image = image.to(device)\n",
    "        B,C, H, W = image.shape\n",
    "        NUM_PATCHES = int(H*W/(patch_size**2))\n",
    "        patches = image.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size).to(device)\n",
    "        patches = patches.permute(0,2, 3, 1, 4, 5).reshape(B, NUM_PATCHES,C, patch_size, patch_size).to(device)\n",
    "        return patches\n",
    "\n",
    "    def sample_context_blocks(self,patches, m=49, image_size=(128,128), patch_size=16 ):\n",
    "        \"\"\"\n",
    "        Randomly selects m continuous patches forming a square block (LxL).\n",
    "\n",
    "        Args:\n",
    "            patches (torch.Tensor): Tensor of patches of shape (num_patches, C, patch_size, patch_size).\n",
    "            image_size (tuple): Original image size as (H, W).\n",
    "            patch_size (int): Size of each patch (patch_size x patch_size).\n",
    "            m (int): Number of continuous patches to select.\n",
    "\n",
    "        Returns:\n",
    "            list: Indices of selected patches.\n",
    "            torch.Tensor: Selected patches of shape (m, C, patch_size, patch_size).\n",
    "        \"\"\"\n",
    "        H, W = image_size\n",
    "        n_patches_h = H // patch_size\n",
    "        n_patches_w = W // patch_size\n",
    "        patches = patches.to(device)\n",
    "        # Compute side length of the square block (LxL = m)\n",
    "        side_length = int(m ** 0.5)\n",
    "        assert side_length ** 2 == m, \"m must be a perfect square to form a square block.\"\n",
    "\n",
    "        # Randomly select a top-left corner for the square block\n",
    "        max_row = n_patches_h - side_length\n",
    "        max_col = n_patches_w - side_length\n",
    "        start_row = random.randint(0, max_row)\n",
    "        start_col = random.randint(0, max_col)\n",
    "\n",
    "        # Collect indices of the patches in the square block\n",
    "        context_indices = []\n",
    "        for _ in range(patches.shape[0]):\n",
    "            context_indices.append([\n",
    "            (start_row + i) * n_patches_w + (start_col + j)\n",
    "            for i in range(side_length)\n",
    "            for j in range(side_length)\n",
    "        ])\n",
    "\n",
    "        masked_patches = patches.clone().to(device)\n",
    "        for j in range(patches.shape[0]):\n",
    "            for i in range(len(patches[j])):\n",
    "                if i not in context_indices[j]:\n",
    "                    masked_patches[j][i] = torch.zeros_like(masked_patches[j][i])  # Mask unselected patches to zero\n",
    "\n",
    "        return masked_patches, context_indices\n",
    "\n",
    "    def sample_target_blocks(self,patches, m =9,  image_size=(128,128), patch_size=16):\n",
    "        \"\"\"\n",
    "        Randomly selects m continuous patches forming a square block (LxL).\n",
    "\n",
    "        Args:\n",
    "            patches (torch.Tensor): Tensor of patches of shape (num_patches, C, patch_size, patch_size).\n",
    "            image_size (tuple): Original image size as (H, W).\n",
    "            patch_size (int): Size of each patch (patch_size x patch_size).\n",
    "            m (int): Number of continuous patches to select.\n",
    "\n",
    "        Returns:\n",
    "            list: Indices of selected patches.\n",
    "            torch.Tensor: Selected patches of shape (m, C, patch_size, patch_size).\n",
    "        \"\"\"\n",
    "        patches = patches.to(device)\n",
    "        H, W = image_size\n",
    "        n_patches_h = H // patch_size\n",
    "        n_patches_w = W // patch_size\n",
    "        ar = (1.5 - 0.75) * torch.rand((1)) + 0.75 # aspect ratio\n",
    "\n",
    "        # Compute side length of the target block\n",
    "        side_length_h = int(ar*(m)**(0.5))\n",
    "        side_length_w = int(m/side_length_h)\n",
    "\n",
    "        # Randomly select a top-left corner for the square block\n",
    "        max_row = n_patches_h - side_length_h\n",
    "        max_col = n_patches_w - side_length_w\n",
    "        start_row = random.randint(0, max_row)\n",
    "        start_col = random.randint(0, max_col)\n",
    "\n",
    "        # Collect indices of the patches in the square block\n",
    "        target_indices = []\n",
    "        for _ in range(patches.shape[0]):\n",
    "            target_indices.append([\n",
    "            (start_row + i) * n_patches_w + (start_col + j)\n",
    "            for i in range(side_length_h)\n",
    "            for j in range(side_length_w)\n",
    "        ])\n",
    "\n",
    "        masked_patches = patches.clone().to(device)\n",
    "        for j in range(patches.shape[0]):\n",
    "            for i in range(len(patches[j])):\n",
    "                if i not in target_indices[j]:\n",
    "                    masked_patches[j][i] = torch.zeros_like(masked_patches[j][i])  # Mask unselected patches to zero\n",
    "\n",
    "        return masked_patches, target_indices\n",
    "\n",
    "    def remove_overlaps(self,context_blocks, context_indices, NUM_TARGETS=4):\n",
    "\n",
    "        TARGET_INDICES = []\n",
    "        for i in  range(NUM_TARGETS):\n",
    "            target,indices = sample_target_blocks(patches)\n",
    "            target = target.to(device)\n",
    "            TARGET_INDICES.append(indices)\n",
    "\n",
    "            if i == 0 :\n",
    "                TARGET_BLOCKS = target.clone().to(device)\n",
    "            else:\n",
    "                TARGET_BLOCKS = torch.cat((TARGET_BLOCKS,target),dim=1).to(device)\n",
    "\n",
    "            for i in range(len(indices)):\n",
    "                for idx in indices[i]:\n",
    "                    context_indices_i = set(context_indices[i])\n",
    "                    if idx  in context_indices_i:\n",
    "                        context_blocks[i][idx] = torch.zeros_like(context_blocks[i][idx])\n",
    "                        context_indices[i].remove(idx)\n",
    "            context_blocks, TARGET_BLOCKS = context_blocks.to(device), TARGET_BLOCKS.to(device)\n",
    "        return context_blocks, context_indices ,TARGET_BLOCKS.reshape(-1,4,64,3,16,16), TARGET_INDICES\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        patches = self.divide_into_patches(x)\n",
    "        context, context_indices = self.sample_context_blocks(patches)\n",
    "        context, context_indices,targets, target_indices  = self.remove_overlaps(context, context_indices)\n",
    "        return context, context_indices, targets, target_indices\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code taken from - https://github.com/facebookresearch/ijepa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class GetPositionalEmbeddings(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GetPositionalEmbeddings, self).__init__()\n",
    "\n",
    "    def get_1d_sincos_pos_embed_from_grid(self,embed_dim, pos):\n",
    "        \"\"\"\n",
    "        embed_dim: output dimension for each position\n",
    "        pos: a list of positions to be encoded: size (M,)\n",
    "        out: (M, D)\n",
    "        \"\"\"\n",
    "        assert embed_dim % 2 == 0\n",
    "        omega = np.arange(embed_dim // 2, dtype=float)\n",
    "        omega /= embed_dim / 2.\n",
    "        omega = 1. / 10000**omega   # (D/2,)\n",
    "\n",
    "        pos = pos.reshape(-1)   # (M,)\n",
    "        out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
    "\n",
    "        emb_sin = np.sin(out)  # (M, D/2)\n",
    "        emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "        emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "        return emb\n",
    "\n",
    "    def get_1d_sincos_pos_embed(self,embed_dim, grid_size, cls_token=False):\n",
    "        \"\"\"\n",
    "        grid_size: int of the grid length\n",
    "        return:\n",
    "        pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "        \"\"\"\n",
    "        grid = np.arange(grid_size, dtype=float)\n",
    "        pos_embed = self.get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "        if cls_token:\n",
    "            pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "        return pos_embed\n",
    "\n",
    "\n",
    "    def get_2d_sincos_pos_embed_from_grid(self,embed_dim, grid):\n",
    "        assert embed_dim % 2 == 0\n",
    "\n",
    "        # use half of dimensions to encode grid_h\n",
    "        emb_h = self.get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "        emb_w = self.get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "        emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "        return emb\n",
    "\n",
    "    def get_2d_sincos_pos_embed(self,embed_dim, grid_size, cls_token=False):\n",
    "        \"\"\"\n",
    "        grid_size: int of the grid height and width\n",
    "        return:\n",
    "        pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "        \"\"\"\n",
    "        grid_h = np.arange(grid_size, dtype=float)\n",
    "        grid_w = np.arange(grid_size, dtype=float)\n",
    "        grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "        grid = np.stack(grid, axis=0)\n",
    "\n",
    "        grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "        pos_embed = self.get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "        if cls_token:\n",
    "            pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "        return pos_embed\n",
    "\n",
    "    def forward(self, grid_size):\n",
    "        pos_emb = self.get_2d_sincos_pos_embed(768, grid_size)\n",
    "        return pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "get_positional_embeddings = GetPositionalEmbeddings().to(device)\n",
    "patchembedder  = GetPatchEmbeddings().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.query(query)  # (batch_size, seq_len, embed_dim)\n",
    "        K = self.key(key)      # (batch_size, seq_len, embed_dim)\n",
    "        V = self.value(value)  # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # Split into multiple heads\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        energy = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1).to(device)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        out = torch.matmul(attention, V).to(device)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # Concatenate heads and pass through final linear layer\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n",
    "        out = self.fc_out(out)  # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hid_dim):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hid_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x ):\n",
    "        # Attention\n",
    "        attention_out = self.attention(x, x, x).to(device)\n",
    "        x = self.norm1(attention_out + x)  # Residual connection\n",
    "\n",
    "        # Feed-forward network\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(ffn_out + x)  # Residual connection\n",
    "\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,patchembedder,positionembedder,  patch_size, in_channels, embed_dim, num_heads,\n",
    "                 num_layers, ff_hid_dim, max_len: int = 512, target = False):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_embedding = patchembedder\n",
    "        self.position_embedding = positionembedder\n",
    "        self.target = target\n",
    "\n",
    "        # Transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, num_heads, ff_hid_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ]).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, target_rep: Optional[torch.Tensor] = None, target_indices :Optional [list] = None):\n",
    "        # Step 1: Patch embedding\n",
    "        if self.target == False:\n",
    "            batch_size = x.size(0)\n",
    "            x,_,target,target_indices = self.patch_embedding(x) # (B, embed_dim, H/patch_size, W/patch_size)\n",
    "\n",
    "            x = x.flatten(2) # (B, num_patches, embed_dim)\n",
    "\n",
    "            # Step 2: Add positional embedding\n",
    "            pos_emb = torch.from_numpy(self.position_embedding(8)).unsqueeze(dim=0).repeat_interleave(repeats = 32, dim=0).to(device)\n",
    "            pos_emb = pos_emb.float()\n",
    "            x = x +  pos_emb\n",
    "\n",
    "            # Step 3: Pass through transformer layers\n",
    "            for layer in self.encoder_layers:\n",
    "                x = layer(x)\n",
    "\n",
    "            # Step 4: Return output for each patch\n",
    "            x, target  =x.to(device), target.to(device)\n",
    "            return x , target, target_indices # (B, num_patches, embed_dim)\n",
    "        else:\n",
    "            batch_size = x.size(0)\n",
    "            target_representations = []\n",
    "            target_blocks = target_rep.to(device)\n",
    "            for i in range(target_blocks.shape[1]):\n",
    "                x = target_blocks[:,i,:,:].flatten(2) # (B, num_patches, embed_dim)\n",
    "\n",
    "                # Step 2: Add positional embedding\n",
    "                pos_emb = torch.from_numpy(self.position_embedding(8)).unsqueeze(dim=0).repeat_interleave(repeats = 32, dim=0).to(device)\n",
    "                pos_emb = pos_emb.float()\n",
    "                x = x+  pos_emb\n",
    "\n",
    "                # Step 3: Pass through transformer layers\n",
    "                for layer in self.encoder_layers:\n",
    "                    x = layer(x)\n",
    "\n",
    "                target_representations.append(x)\n",
    "            return target_representations  # (B, num_patches, embed_dim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "context_enocder = VisionTransformer(\n",
    "        patchembedder,\n",
    "        get_positional_embeddings,\n",
    "        image_size=128,\n",
    "        patch_size=16,\n",
    "        in_channels=3,\n",
    "        embed_dim=768,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        ff_hid_dim=768,\n",
    "        dropout=0.1,\n",
    "        target=False\n",
    "    )\n",
    "target_encoder = VisionTransformer(\n",
    "        patchembedder,\n",
    "        get_positional_embeddings,\n",
    "        image_size=128,\n",
    "        patch_size=16,\n",
    "        in_channels=3,\n",
    "        embed_dim=768,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        ff_hid_dim=768,\n",
    "        dropout=0.1,\n",
    "        target=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class VisionTransformer_predictor(nn.Module):\n",
    "    def __init__(self,positionembedder, patch_size, image_size, in_channels, embed_dim, num_heads,\n",
    "                 num_layers, ff_hid_dim, max_len: int = 512):\n",
    "        super(VisionTransformer_predictor, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.position_embedding = positionembedder.to(device)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, num_heads, ff_hid_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ]).to(device)\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 768)\n",
    "        )\n",
    "    def get_mask_embeddings(self,target_list : list):\n",
    "\n",
    "        pos_emb = torch.from_numpy(self.position_embedding(8)).unsqueeze(dim=0).repeat_interleave(repeats = 32, dim=0).to(device)\n",
    "        pos_emb = pos_emb.float()\n",
    "        for i in range(len(target_list)):\n",
    "            pos_masks = self.position_embedding.get_1d_sincos_pos_embed_from_grid(768, pos = np.array(target_list[i]))\n",
    "            pos_masks = torch.from_numpy(pos_masks).to(device)\n",
    "            pos_masks = torch.cat([pos_masks, torch.zeros(pos_emb[i].shape[0] - pos_masks.shape[0], 768).to(device)], dim=0).to(device)\n",
    "            pos_emb[i] =  pos_emb[i] + pos_masks\n",
    "        return pos_emb\n",
    "\n",
    "    def forward(self, x: torch.Tensor, target_lists: list):\n",
    "        x = x.to(device)\n",
    "        pos_emb = self.get_mask_embeddings(target_lists)\n",
    "        pos_emb = pos_emb.float()\n",
    "        x = x + pos_emb\n",
    "\n",
    "        # Step 3: Pass through transformer layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "predictor = VisionTransformer_predictor(\n",
    "        get_positional_embeddings,\n",
    "        image_size=128,\n",
    "        patch_size=16,\n",
    "        in_channels=3,\n",
    "        embed_dim=768,\n",
    "        num_heads=4,\n",
    "        num_layers=6,\n",
    "        ff_hid_dim=384,\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(context, target_representaions, target_indices, predictor_model):\n",
    "    loss = 0\n",
    "    for i in range(len(target_representaions)):\n",
    "        context_representations = predictor_model(context.detach(), target_indices[i])\n",
    "        loss  += torch.mean(torch.sqrt(torch.sum(torch.square(context_representations-target_representaions[i]))))\n",
    "\n",
    "    return loss/len(target_representaions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_iJEPA(context_model, target_model, predictor_model, num_epochs = 10):\n",
    "    context_model  = context_model.to(device)\n",
    "    target_model = target_model.to(device)\n",
    "    predictor_model = predictor_model.to(device)\n",
    "    dataloader = torch.utils.data.DataLoader(animal_dataset, batch_size =32 , shuffle = True, num_workers=True)\n",
    "    context_opt = torch.optim.Adam(context_model.parameters(), lr = 1e-4)\n",
    "    predictor_opt = torch.optim.Adam(predictor_model.parameters(), lr  =1e-4)\n",
    "\n",
    "    for param in target_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    train_losses = []\n",
    "    context_model.train()\n",
    "    predictor_model.train()\n",
    "    for epochs in range(num_epochs):\n",
    "        avg_loss = 0\n",
    "        for x,_ in tqdm(dataloader):\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "                x = x.to(device)\n",
    "                context_opt.zero_grad()\n",
    "                predictor_opt.zero_grad()\n",
    "                context, target, target_indices = context_model(x)\n",
    "                with torch.no_grad():\n",
    "                    target_representations = target_model(x,target, target_indices)\n",
    "                loss = loss_fn(context.detach(), target_representations, target_indices, predictor_model)\n",
    "                avg_loss += loss.item()\n",
    "\n",
    "                loss.backward(retain_graph = True, create_graph  =True)\n",
    "                context_opt.step()\n",
    "                predictor_opt.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for param_q, param_k in zip(context_model.parameters(), target_model.parameters()):\n",
    "                        param_k.data.mul_(0.996).add_((1.-0.996) * param_q.detach().data)\n",
    "\n",
    "        avg_loss/=len(dataloader)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"epoch:{epochs} | avg_loss: {avg_loss}\")\n",
    "\n",
    "\n",
    "    print('finished Training')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
